{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2UJbXshixx-"
   },
   "source": [
    "## Project - PyTorch for Deep Learning\n",
    "\n",
    "##### First Name :\n",
    "\n",
    "##### Last Name :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHcCtf8dixx_"
   },
   "source": [
    "### Principles of PyTorch\n",
    "\n",
    "PyTorch provides two main features:\n",
    "\n",
    "- It enables tensor and vector computations on the GPU with an API similar to NumPy.\n",
    "- It automatically records all computations to support backpropagation (or automatic differentiation). Given a sequence of operations from a tensor $\\theta$ to define a scalar $g(\\theta)$, it can compute $\\nabla_\\theta g(\\theta)$ precisely (somehow ``exactly\") with a single function call.\n",
    "\n",
    "Typically, $\\theta$ represents a neural network parameter, and $g$ is a loss function, such as $\\ell(f_{\\theta}(x), y)$ in supervised learning.\n",
    "\n",
    "In PyTorch, each node in the computation graph is dynamically created during the forward pass within the Python interpreter, allowing easy transfer of NumPy code to PyTorch and preserving all flow control operations (e.g., loops, if/else statements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vzSjQx3bOsE"
   },
   "source": [
    "### Setting up the device\n",
    "PyTorch can operate on either a CPU or a GPU, as outlined below. To use a GPU for this project, you may consider using Google Colab. You can also use CPU with your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2bsxEbDixyA",
    "outputId": "7865423b-7a08-4b3a-cda1-c292652d478d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Two options: 'cuda' (for GPU) or 'cpu' (if no GPU available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Device type set:\", \"GPU\" if device.type == \"cuda\" else \"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj2vC0NpixyA"
   },
   "source": [
    "## PART I - A first example\n",
    "\n",
    "Let us define the norm function $xâ†’\\|x\\|$ on $\\mathbb{R}^n$ using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYIe0v1VbOsF",
    "outputId": "616dc35a-d8e7-4e12-d96a-612490785a95"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "x = torch.rand(n, dtype=torch.float32)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOXjOPVJixyC"
   },
   "source": [
    "Now define a norm function that returns the Euclidean norm of a vector $x \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$f(x) = || x ||_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGu95IavbOsG"
   },
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXwJHV8kixyC"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7V2Ue9ZyixyC"
   },
   "source": [
    "We are interested in computing $\\nabla_x f(x) = \\frac{x}{|| x ||_2}$.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Show rigorously that if $f(x) = || x ||_2$ then $\\nabla_x f(x) = \\frac{x}{|| x ||_2}$.\n",
    "\n",
    "\n",
    "Assume we are too lazy to differentiate to obtain the analytical form of the gradient... Instead we use the `autograd` facilities of PyTorch which is an implemented form of automatic differentiation (and backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1wsY1i0bOsG"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdGkrUNcixyD"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Write a function the pytorch code for function `g` that computes the cosine of the angle between two vectors with float entries $\\mathbf{x}$ and $\\mathbf{y}$:\n",
    "\n",
    "$$g(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x}^T \\mathbf{y}}{|| \\mathbf{x} ||_2 || \\mathbf{y} ||_2 }$$\n",
    "\n",
    "- Use `torch.autograd` to compute the derivatives with respect to $\\mathbf{x} \\in \\mathbb{R}^3$ and $\\mathbf{y} \\in \\mathbb{R}^3$ for some random values of your choice\n",
    "\n",
    "- Compute $\\nabla_x g(x, y)$ and $\\nabla_y g(x, y)$ for some choice of $\\mathbf{x} = \\alpha \\cdot \\mathbf{y}$ with any $\\alpha \\in \\mathbb{R}$. Check that you can get the expected result with Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uDFn_6nIixyE",
    "outputId": "4ec96bc0-77f9-405d-ef46-bd3fbae602b1"
   },
   "outputs": [],
   "source": [
    "def g(x, y):\n",
    "    # TODO: fix the following code to implement the cosine similarity function instead.\n",
    "    return torch.sum(x) + torch.sum(y)\n",
    "\n",
    "\n",
    "x = torch.tensor([0, 1, 2], dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([3, 0.9, 2.2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "cosine = g(x, y)\n",
    "cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPQdk7L4ixyE"
   },
   "source": [
    "Let's reinitialize our two variables to non colinear values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZllJi8PixyE"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([0, 1, 2], dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([3, 0.9, 2.2], dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc1In4IEixyE"
   },
   "source": [
    "Execute the following cells many times (use `ctrl-enter`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nKQ2raKixyE",
    "outputId": "73af49a0-843b-4680-852f-934d94b01b47"
   },
   "outputs": [],
   "source": [
    "cosine = g(x, y)\n",
    "print('cosine:', cosine)\n",
    "cosine.backward()\n",
    "with torch.no_grad():\n",
    "    # Do not record the following operations for future calls to cosine.backward().\n",
    "    x.add_(0.5 * x.grad)\n",
    "    y.add_(0.5 * y.grad)\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"x/y:\", x / y)\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"y.grad:\", y.grad)\n",
    "x.grad.zero_()\n",
    "y.grad.zero_();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJ6A6x3LixyE"
   },
   "source": [
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10H5IoccixyE"
   },
   "source": [
    "## PART II - Comparing gradient descent methods\n",
    "\n",
    "In this part, we use PyTorch to compare the different gradient methods and a toy 2D examples: we will try to find the minimum of the difference of two Gaussians. PyTorch provides a convenient wrapper named `nn.Module` to define parametrized functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efsaFTQRixyE"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class Norm(nn.Module):\n",
    "\n",
    "    def __init__(self, p=2.):\n",
    "        super(Norm, self).__init__()\n",
    "        self.p = Parameter(torch.tensor([p], dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        p_sum = torch.sum(torch.pow(x, self.p), dim=0)\n",
    "        return torch.pow(p_sum, 1 / self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TN3zSllaLSjb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzglJfmqixyE"
   },
   "source": [
    "In the above definition, we treat the attribute `p` of the \"p-norm\" (a generalization of the Euclidean norm) as a parameter that can be differentiated upon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Swv9I7AzixyE"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "x = torch.rand(n)\n",
    "norm = Norm(p=3.)\n",
    "v = norm(x)\n",
    "v.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExWHkcuDixyE"
   },
   "source": [
    "We can access $\\nabla_p(x \\to || x ||_p)$ in `norm.p.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQRIv83QixyE",
    "outputId": "f6a56b88-2dfe-44db-b9b4-d9bc0a33a613"
   },
   "outputs": [],
   "source": [
    "norm.p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGoC5f6IixyE"
   },
   "source": [
    "We now define a Gaussian operator, along with a generic Gaussian combination. We will not consider the gradient w.r.t the parameters of these modules, hence we specify `requires_grad=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3lD-6gEixyE"
   },
   "outputs": [],
   "source": [
    "class Gaussian(nn.Module):\n",
    "\n",
    "    def __init__(self, precision, mean):\n",
    "        super(Gaussian, self).__init__()\n",
    "\n",
    "        assert precision.shape == (2, 2)\n",
    "        assert mean.shape == (2,)\n",
    "\n",
    "        self.precision = Parameter(precision, requires_grad=False)\n",
    "        self.mean = Parameter(mean, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute the (unormalized) likelihood of x given a Gaussian.\n",
    "\n",
    "        x can be a 2D-vector or a batch of 2D-vectors.\n",
    "        \"\"\"\n",
    "        xc = x - self.mean\n",
    "        if len(xc.shape) == 1:\n",
    "            # Reshape xc to work as a mini-batch of one vector.\n",
    "            xc = xc.view(1, -1)\n",
    "        return torch.exp(-.5 * (torch.sum((xc @ self.precision) * xc, dim=1)))\n",
    "\n",
    "\n",
    "class GaussianCombination(nn.Module):\n",
    "\n",
    "    def __init__(self, precisions, means, weights):\n",
    "        super(GaussianCombination, self).__init__()\n",
    "        assert len(precisions) == len(means) == len(weights)\n",
    "        self.gaussians = nn.ModuleList()\n",
    "        for precision, mean in zip(precisions, means):\n",
    "            self.gaussians.append(Gaussian(precision, mean))\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        return sum(w * g(x) for g, w in zip(self.gaussians, self.weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OJ9lnGAixyE"
   },
   "source": [
    "We now define $f(x) = \\exp(-(x- m_1)^T P_1 (x - m_1)) - \\exp(-(x- m_2)^T P_2 (x - m_2))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMPItdzhixyE"
   },
   "outputs": [],
   "source": [
    "p1 = torch.tensor([[1, 0], [0, 4]], dtype=torch.float32)\n",
    "m1 = torch.tensor([0, 1], dtype=torch.float)\n",
    "w1 = 1\n",
    "p2 = torch.tensor([[1, -2], [-2, 10]], dtype=torch.float32)\n",
    "m2 = torch.tensor([0, -2], dtype=torch.float32)\n",
    "w2 = -1\n",
    "\n",
    "f = GaussianCombination([p1, p2], [m1, m2], [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KIPtCzPixyE"
   },
   "source": [
    "We define a plotting function to visualize $f$. Note the small boilerplate to interface PyTorch with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ar-GzgZ5ixyF"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_function(f, ax):\n",
    "    x_max, y_max, x_min, y_min = 3, 3, -3, -3\n",
    "    x = np.linspace(x_min, x_max, 100, dtype=np.float32)\n",
    "    y = np.linspace(y_min, y_max, 100, dtype=np.float32)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    samples = np.concatenate((X[:, :, None], Y[:, :, None]), axis=2)\n",
    "    samples = samples.reshape(-1, 2)\n",
    "    samples = torch.from_numpy(samples).requires_grad_()\n",
    "    Z = f(samples).data.numpy()\n",
    "    Z = Z.reshape(100, 100)\n",
    "    CS = ax.contour(X, Y, Z)\n",
    "    ax.clabel(CS, inline=1, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lWUFIYFixyF",
    "outputId": "206522cc-1eb1-49cd-a54a-894dd2141a02"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_function(f, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edKFpt_NixyF"
   },
   "source": [
    "We will now try to minimize $f$ using gradient descent (including optional features). For this, we define a minimize function that performs gradient descent, along with a helper class `GradientDescent` that will perform the updates given the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxV09Xe_ixyF"
   },
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "\n",
    "    def __init__(self, params, lr=0.1):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.params:\n",
    "                param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DV3kQL6aixyF"
   },
   "outputs": [],
   "source": [
    "def minimize(f, optimizer, max_iter=10000, verbose=False):\n",
    "    if hasattr(optimizer, 'params'):\n",
    "        [iterate] = optimizer.params\n",
    "    else:\n",
    "        # For pytorch optimizers.\n",
    "        assert len(optimizer.param_groups) == 1\n",
    "        [iterate] = optimizer.param_groups[0]['params']\n",
    "    iterate_record = []\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # iterate.grad may be non zero. We zero it first:\n",
    "        optimizer.zero_grad()\n",
    "        value = f(iterate)\n",
    "\n",
    "        # Compute the gradient of f with respect to the parameters:\n",
    "        value.backward()\n",
    "        # iterate.grad now holds $\\nabla_x f(x)$\n",
    "        if float(torch.sum(iterate.grad ** 2)) < 1e-6:\n",
    "            if verbose:\n",
    "                print('Converged at iteration %i: '\n",
    "                      'f(x) = %e, x = [%e, %e]'\n",
    "                      % (i, value, iterate[0], iterate[1]))\n",
    "            break\n",
    "\n",
    "        # We store the trajectory of the iterates\n",
    "        iterate_record.append(iterate.data.clone()[None, :])\n",
    "        if verbose and i % 100 == 0:\n",
    "            print('Iteration %i: f(x) = %e, x = [%e, %e]'\n",
    "                  % (i, value, iterate[0], iterate[1]))\n",
    "\n",
    "        # Perform the parameter update step using the gradient\n",
    "        # values:\n",
    "        optimizer.step()\n",
    "    return torch.cat(iterate_record, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bp30N7LixyF"
   },
   "source": [
    "Run the minimization algorithm and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pn7m9MLeixyF",
    "outputId": "541e61ee-db42-4403-d69b-d995af18b3bf"
   },
   "outputs": [],
   "source": [
    "# The extra dimension marked with `None` is used to make it\n",
    "# possible\n",
    "init = torch.tensor([0.8, 0.8], dtype=torch.float32)\n",
    "optimizer = GradientDescent([init.clone().requires_grad_()], lr=0.01)\n",
    "iterate_rec = minimize(f, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuiWCcn_ixyF"
   },
   "outputs": [],
   "source": [
    "def plot_trace(iterate_rec, ax, label='', tags=True):\n",
    "    iterate_rec = iterate_rec.numpy()\n",
    "    n_steps = len(iterate_rec)\n",
    "    line = ax.plot(iterate_rec[:, 0], iterate_rec[:, 1], linestyle=':',\n",
    "                   marker='o', markersize=2,\n",
    "                   label=label + \" (%d steps)\" % n_steps)\n",
    "    color = plt.getp(line[0], 'color')\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", ec=color, fc='white',\n",
    "                      lw=1)\n",
    "    if tags:\n",
    "        # Put tags every 10 iterates. The small randomization makes it\n",
    "        # easier to spot tag overlap when the steps are very small.\n",
    "        for i in range(0, len(iterate_rec), 10):\n",
    "            ax.annotate(i, xy=(iterate_rec[i, 0], iterate_rec[i, 1]),\n",
    "                        xycoords='data',\n",
    "                        xytext=(5 + np.random.uniform(-2, 2),\n",
    "                                5 + np.random.uniform(-2, 2)),\n",
    "                        textcoords='offset points',\n",
    "                        bbox=bbox_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnubEvRPixyF",
    "outputId": "6b6fec9c-5882-4bfa-eb15-d5b9bec7da2d"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "plot_function(f, ax1)\n",
    "plot_function(f, ax2)\n",
    "plot_trace(iterate_rec, ax1, label='gradient_descent')\n",
    "plot_trace(iterate_rec, ax2, label='gradient_descent', tags=False)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD8aY_M2ixyF",
    "outputId": "34ec94bd-42f6-4086-d01b-964271205936"
   },
   "outputs": [],
   "source": [
    "plt.plot([f(x).numpy() for x in iterate_rec])\n",
    "plt.xlabel('optimization step')\n",
    "plt.ylabel('objective function value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdpilqxAixyF"
   },
   "source": [
    "### Exercices\n",
    "\n",
    "- Try to move the initialization point to the other side of the yellow mountain, for instance at position `[0.8, 1.2]`. What do you observe? How to do you explain this?\n",
    "\n",
    "- Put back the init to `[0.8, 0.8]` and implement the step method of `MomemtumGradientDescent` in the following cell,\n",
    "- Check that it behaves as `GradientDescent` for `momemtum=0`\n",
    "- Can you find a value of `momentum` that makes it converge faster than gradient descent on for this objective function? Try with different values of `lr` and `momentum`.\n",
    "- Add [torch.optim.Adam](http://pytorch.org/docs/main/generated/torch.optim.Adam.html#torch.optim.Adam) and [torch.optim.RMSprop](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) in the minimization loop.\n",
    "- Compare the four trajectories.\n",
    "\n",
    "\n",
    "For the momentum let us use the following 2-stage mathematical description of the update step:\n",
    "\n",
    "$$ \\mathbf{v} \\leftarrow \\mu \\cdot \\mathbf{v} + \\nabla_\\mathbf{x}\\ell(\\mathbf{x}) $$\n",
    "$$ \\mathbf{x} \\leftarrow \\eta \\cdot \\mathbf{v} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\eta$ is the learning rate coefficient (`lr` in Python code);\n",
    "- $\\mu$ is the momentum coefficient (set to 0.9 by default);\n",
    "- $\\nabla_\\mathbf{x}\\ell(\\mathbf{x})$ is the gradient of the loss function for the current value of the parameters $\\mathbf{x}$;\n",
    "- $\\mathbf{v}$ is the tensor of velocities and as the same shape as the parameters tensor $\\theta$. $v$ is initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34gag6vxixyF"
   },
   "outputs": [],
   "source": [
    "class MomentumGradientDescent(GradientDescent):\n",
    "\n",
    "    def __init__(self, params, lr=0.1, momentum=.9):\n",
    "        super(MomentumGradientDescent, self).__init__(params, lr)\n",
    "        self.momentum = momentum\n",
    "        self.velocities = [torch.zeros_like(param, requires_grad=False)\n",
    "                           for param in params]\n",
    "\n",
    "    def step(self):\n",
    "        # TODO: implement me!\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ep-Pc5uqixyF",
    "outputId": "c8dd851b-d908-4d87-d5e4-212f18d307e7"
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "init = torch.FloatTensor([0.8, 0.8])\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = GradientDescent([init.clone().requires_grad_()], lr=lr)\n",
    "iterate_rec_gd = minimize(f, optimizer)\n",
    "\n",
    "# Momentum\n",
    "optimizer = MomentumGradientDescent([init.clone().requires_grad_()],\n",
    "                                    lr=lr, momentum=0.0)\n",
    "iterate_rec_mom = minimize(f, optimizer)\n",
    "\n",
    "\n",
    "#optimizer_adam = # TODO\n",
    "#iterate_rec_adam = minimize(f, optimizer_adam)\n",
    "\n",
    "\n",
    "#optimizer_rmsprop = # TODO\n",
    "#iterate_rec_rmsprop = minimize(f, optimizer_rmsprop)\n",
    "\n",
    "# Plotting\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16, 8))\n",
    "\n",
    "# Plot the function and the optimization paths\n",
    "plot_function(f, ax0)\n",
    "plot_trace(iterate_rec_gd, ax0, label='gradient descent', tags=False)\n",
    "plot_trace(iterate_rec_mom, ax0, label='momentum', tags=False)\n",
    "#plot_trace(iterate_rec_adam, ax0, label='adam', tags=False)\n",
    "#plot_trace(iterate_rec_rmsprop, ax0, label='rmsprop', tags=False)\n",
    "ax0.legend()\n",
    "\n",
    "# Plot the objective function value over optimization steps\n",
    "ax1.plot([f(x).item() for x in iterate_rec_gd], label='gradient descent')\n",
    "ax1.plot([f(x).item() for x in iterate_rec_mom], label='momentum')\n",
    "#ax1.plot([f(x).item() for x in iterate_rec_adam], label='adam')\n",
    "#ax1.plot([f(x).item() for x in iterate_rec_rmsprop], label='rmsprop')\n",
    "ax1.set_xlabel('optimization step')\n",
    "ax1.set_ylabel('objective function value')\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9V7VLEKixyF"
   },
   "source": [
    "What do you observe ? Try changing the momentum and the initialization to compare optimization traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA2aFfgrbOsI"
   },
   "source": [
    "## PART III - Fashion MNIST classification using PyTorch\n",
    "\n",
    "In this part, we will try to classify the Fashion MNIST dataset\n",
    "(https://github.com/zalandoresearch/fashion-mnist) using VGG-like architectures (https://arxiv.org/abs/1409.1556). This part is inspired from the MNIST example from PyTorch (https://github.com/pytorch/examples/tree/master/mnist), and introduce tricks to automatically tune and schedule the learning rate for SGD (see this course's slides, https://arxiv.org/abs/1506.01186, and FastAI course for example http://fastai.org).\n",
    "\n",
    "## Fashion MNIST\n",
    "\n",
    "This 10 class dataset is a drop-in replacement for MNIST with clothes instead of digits.\n",
    "\n",
    "<img src=\"https://api.wandb.ai/files/kylegtest/fashion-mnist-gan/up6zul3y/media/images/Generator%20Output_1396.png?height=55\" alt=\"fashion_mnist\" height=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_ORzORrbOsI"
   },
   "source": [
    "Let's import a few functions first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9N37xKgbOsI"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.cm import get_cmap\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qMP9q_sbOsI"
   },
   "source": [
    "Some system/model hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EvhlaN_bOsI"
   },
   "outputs": [],
   "source": [
    "cuda = False\n",
    "batch_size = 128\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3fwqQMLbOsI"
   },
   "outputs": [],
   "source": [
    "english_labels = [\n",
    "                  \"Trouser\",\n",
    "                  \"T-shirt/top\",\n",
    "                  \"Pullover\",\n",
    "                  \"Dress\",\n",
    "                  \"Coat\",\n",
    "                  \"Sandal\",\n",
    "                  \"Shirt\",\n",
    "                  \"Sneaker\",\n",
    "                  \"Bag\",\n",
    "                  \"Ankle boot\"]\n",
    "\n",
    "train_data = datasets.FashionMNIST('data', train=True, download=True,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                   ]))\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcWK7Wv2bOsI"
   },
   "source": [
    "Let's compute the average mean and std of the train images. We will use them for normalizing data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1GU15nybOsI"
   },
   "outputs": [],
   "source": [
    "n_samples_seen = 0.\n",
    "mean = 0\n",
    "std = 0\n",
    "for train_batch, train_target in train_loader:\n",
    "    batch_size = train_batch.shape[0]\n",
    "    train_batch = train_batch.view(batch_size, -1)\n",
    "    this_mean = torch.mean(train_batch, dim=1)\n",
    "    this_std = torch.sqrt(\n",
    "        torch.mean((train_batch - this_mean[:, None]) ** 2, dim=1))\n",
    "    mean += torch.sum(this_mean, dim=0)\n",
    "    std += torch.sum(this_std, dim=0)\n",
    "    n_samples_seen += batch_size\n",
    "\n",
    "mean /= n_samples_seen\n",
    "std /= n_samples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G09N9CYHbOsI",
    "outputId": "113b565b-4c37-4b19-90c4-15833cb157a8"
   },
   "outputs": [],
   "source": [
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lx2mR6HbOsI"
   },
   "source": [
    "We now reload the data with a further `Normalize` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8THvH2UIbOsI"
   },
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST('data', train=True, download=False,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=mean.view(1),\n",
    "                                                            std=std.view(1))]))\n",
    "\n",
    "test_data = datasets.FashionMNIST('data', train=False, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean.view(1),\n",
    "                                                           std=std.view(1))]))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32,\n",
    "                                          shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN2y_o6gbOsI"
   },
   "source": [
    "We define a very simple model, suitable for CPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzR6B2EsbOsI"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=(3, 3), padding=1)\n",
    "        self.dropout_2d = nn.Dropout2d(p=0.25)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 20, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout_2d(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = self.dropout_2d(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = x.view(-1, 7 * 7 * 20)  # flatten / reshape\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.fc1.reset_parameters()\n",
    "        self.fc2.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4b37HLcbOsI",
    "outputId": "23f606c8-51f6-4a55-c058-8e4e519cd61d"
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfXAw8BAbOsI"
   },
   "source": [
    "We can plot a training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9IY1StibOsI",
    "outputId": "6752f298-2bf0-43d7-d904-11945e79fc69"
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "img, target = train_data[1]\n",
    "# n_channel, width, height\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-PiOw-LbOsI",
    "outputId": "8b53df65-cdf3-4133-9dc3-abb2a49f68e4"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(img[0].numpy(), cmap=get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnDNbRWdbOsI",
    "outputId": "4b893281-1004-41b7-bb8b-9154e5186b83"
   },
   "outputs": [],
   "source": [
    "pred = model(img[None, :])\n",
    "print(target, english_labels[target])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYcj-_U4bOsI"
   },
   "source": [
    "We define a train loop and a test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6KKNxkabOsI"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        batch_size = data.shape[0]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * batch_size\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    train_loss /= len(test_loader.dataset)\n",
    "    return train_loss\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            # sum up batch loss\n",
    "            _, pred = output.data.max(dim=1)\n",
    "            # get the index of the max log-probability\n",
    "            correct += torch.sum(pred == target.data.long()).item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_accuracy = float(correct) / len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f},'\n",
    "              ' Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * test_accuracy))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3QGETB4bOsJ"
   },
   "source": [
    "The `find_lr` function provides a learning rate for SGD or Adam, following heuristics from https://arxiv.org/abs/1506.01186:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t24fXa7fbOsJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loop_loader(data_loader):\n",
    "    while True:\n",
    "        for elem in data_loader:\n",
    "            yield elem\n",
    "\n",
    "def find_lr(model, train_loader, init_lr, max_lr, steps, n_batch_per_step=30):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=init_lr)\n",
    "    current_lr = init_lr\n",
    "    best_lr = current_lr\n",
    "    best_loss = float('inf')\n",
    "    lr_step = (max_lr - init_lr) / steps\n",
    "\n",
    "    loader = loop_loader(train_loader)\n",
    "    for i in range(steps):\n",
    "        mean_loss = 0\n",
    "        n_seen_samples = 0\n",
    "        for j, (data, target) in enumerate(loader):\n",
    "            if j > n_batch_per_step:\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            mean_loss += loss.item() * data.shape[0]\n",
    "            n_seen_samples += data.shape[0]\n",
    "            optimizer.step()\n",
    "\n",
    "        mean_loss /= n_seen_samples\n",
    "        print('Step %i, current LR: %f, loss %f' % (i, current_lr, mean_loss))\n",
    "\n",
    "        if np.isnan(mean_loss) or mean_loss > best_loss * 4:\n",
    "            return best_lr / 4\n",
    "\n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            best_lr = current_lr\n",
    "\n",
    "        current_lr += lr_step\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "\n",
    "    return best_lr / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjETN5UobOsJ"
   },
   "source": [
    "Let's load our model on the GPU if required. We then define an optimizer and a learning rate scheduler. (It takes 4mn to run with CPU Macbook PRO M1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGykNU07bOsJ",
    "outputId": "1cb65b0e-639c-4b9a-dc99-a76e37b5a777"
   },
   "outputs": [],
   "source": [
    "log_interval = 100\n",
    "epochs = 12\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.reset_parameters()\n",
    "lr = find_lr(model, train_loader, 1e-4, 1, 100, 30)\n",
    "model.reset_parameters()\n",
    "\n",
    "print('Best LR', lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TewoU4KdbOsJ"
   },
   "source": [
    "Let's train our model.\n",
    "\n",
    "**NB:** If training a large model is slow on your local computer, try using Google Colab with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZseATCVbOsJ"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                       T_max=3,\n",
    "                                                       last_epoch=-1)\n",
    "\n",
    "logs = {'epoch': [], 'train_loss': [], 'test_loss': [],\n",
    "        'test_accuracy': [], 'lr': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_2MDkGebOsJ",
    "outputId": "8a0b701b-eb5a-47ff-8f73-101599754771"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, optimizer, train_loader, epoch)\n",
    "    test_loss, test_accuracy = test(model, test_loader)\n",
    "    logs['epoch'].append(epoch)\n",
    "    logs['train_loss'].append(train_loss)\n",
    "    logs['test_loss'].append(test_loss)\n",
    "    logs['test_accuracy'].append(test_accuracy)\n",
    "    logs['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKjmyedSbOsJ"
   },
   "source": [
    "### Exercices\n",
    "\n",
    "- Define a VGG-like model: add more convolutional and max pooling layers to increase the number of channels progressively while decreasing the dimensions of the feature maps with max pooling.\n",
    "- Research and briefly explain what dropout is.\n",
    "- Try to use Adam instead of SGD in conjunction with the `find_lr` heuristic and the cosine learning rate schedule above;\n",
    "- Try to beat the test accuracy of the model ``class Model(nn.Module):`` above ( $\\approx$ 90 %).\n",
    "- (optional) Try data augmentation (horizontal flips, random crops, cutout...);\n",
    "- (optional) Try to use batch-normalization;\n",
    "- (optional) Implement the [mixup stochastic label interpolation](https://arxiv.org/abs/1710.09412);\n",
    "- (optional) Implement skip-connections.\n",
    "\n",
    "See how you compare to other approaches:\n",
    "- https://github.com/zalandoresearch/fashion-mnist\n",
    "- https://www.kaggle.com/zalando-research/fashionmnist\n",
    "\n",
    "**NB:** If training a large model is slow on your local computer, try using Google Colab with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "pjE2AICEbOsJ",
    "outputId": "6c025b24-0d51-4c13-ab37-16842f78059e"
   },
   "outputs": [],
   "source": [
    "class VGGCell(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, depth, max_pooling=True):\n",
    "        super(VGGCell, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                self.convs.append(nn.Conv2d(in_channel, out_channel,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            padding=1))\n",
    "            else:\n",
    "                self.convs.append(nn.Conv2d(out_channel, out_channel,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            padding=1))\n",
    "        self.max_pooling = max_pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        if self.max_pooling:\n",
    "            x = ##TODO\n",
    "        return x\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        vgg1 = VGGCell(1, 32, 2, max_pooling=True)\n",
    "        vgg2 = VGGCell(32, 64, 3, max_pooling=False)\n",
    "        vgg3 = VGGCell(64, 128, 3, max_pooling=True)\n",
    "        vgg4 = VGGCell(128, 256, 3, max_pooling=False)\n",
    "        self.vggs = nn.ModuleList([vgg1, vgg2, vgg3, vgg4])\n",
    "        self.dropout_2d = nn.Dropout2d(p=0.25)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 256, 256)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for vgg in self.vggs:\n",
    "            x = self.dropout_2d(vgg(x))\n",
    "        x = x.view(-1, 7 * 7 * 256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = ##TODO\n",
    "        return ##TODO\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for vgg in self.vggs:\n",
    "            vgg.reset_parameters()\n",
    "        self.fc1.reset_parameters()\n",
    "        self.fc2.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfpWGKCvbOsK"
   },
   "source": [
    "## PART IV\n",
    "\n",
    "Summarize what you have learned from this project and the course in general. Describe the skills you have gained and what you are now able to accomplish with PyTorch."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
